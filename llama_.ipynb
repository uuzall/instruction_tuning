{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoTokenizer, LlamaTokenizer, AddedToken\n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F \n",
    "import torch \n",
    "import transformers.optimization as optim \n",
    "# import torch.optim as optim \n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange, tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "from datasets import load_dataset \n",
    "from accelerate import Accelerator, DeepSpeedPlugin, accelerator\n",
    "import pickle as pkl \n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, PeftModel, PeftConfig, PeftModelForCausalLM, get_peft_config\n",
    "import peft \n",
    "import pandas as pd\n",
    "import transformers\n",
    "import wandb \n",
    "import numpy as np \n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device == 'cuda': \n",
    "  print(torch.cuda.get_device_name()) \n",
    "else:\n",
    "  print(device) \n",
    "\n",
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s> <s> <s> left\n",
      "[1, 2] [1, 1] [1, 1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b343f54cc2b4480ca08a90c745d87a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.12433454005023165\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained('/media/uzal/New Volume/hf_models/Llama-2-7b-hf', unk_token = '<unk>', bos_token = '<s>', eos_token = '</s>', pad_token='<s>', padding_side='left', legacy=False)\n",
    "print(tokenizer.eos_token, tokenizer.bos_token, tokenizer.pad_token, tokenizer.padding_side)\n",
    "print(tokenizer.encode('</s>'), tokenizer.encode('<s>'), tokenizer.encode('<s>'))\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/media/uzal/New Volume/hf_models/Llama-2-7b-hf\", load_in_8bit=True, torch_dtype=torch.float16, device_map='auto', use_cache=False)\n",
    "\n",
    "config = LoraConfig(\n",
    "    peft_type= \"PREFIX_TUNING\",\n",
    "    task_type= \"CAUSAL_LM\",\n",
    "    inference_mode= False,\n",
    "\t\tr = 16, \n",
    "    lora_alpha = 32, \n",
    "    lora_dropout=0.1, \n",
    "    bias='none', \n",
    ")\n",
    "# model.enable_input_require_grads()\n",
    "# config = get_peft_config(config)\n",
    "model = peft.prepare_model_for_kbit_training(model)\n",
    "model = PeftModelForCausalLM(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_json(\"/media/uzal/New Volume/data/ShareGPT_Vicuna_unfiltered/ShareGPT_V3_unfiltered_cleaned_split.json\")\n",
    "# all_data = list() \n",
    "\n",
    "# for conversations in tqdm(data.conversations): \n",
    "# \tsen = ''\n",
    "# \tattn_map = list() \n",
    "# \tfor i in conversations: \n",
    "# \t\tif i['from'] == 'human': \n",
    "# \t\t\tsen += 'Human: ' + i['value'] + '\\n'\n",
    "# \t\t\tattn_map += list(0 for _ in range(len(tokenizer('Human: ' + i['value'] + '\\n').input_ids)))\n",
    "# \t\telse: \n",
    "# \t\t\tsen += 'Assistant: ' + i['value'] + tokenizer.eos_token + '\\n\\n'\n",
    "# \t\t\tattn_map += list(1 for _ in range(len(tokenizer('Assistant: ' + i['value'] + tokenizer.eos_token + '\\n\\n').input_ids)))\n",
    "# \tif sen != '' and len(tokenizer(sen).input_ids) <= MAX_LEN: \n",
    "# \t\tall_data.append((sen, attn_map))\n",
    "\n",
    "# with open('data/share_gpt_512_llama.pkl', 'wb') as file: \n",
    "# \tpkl.dump(all_data, file)\n",
    "\n",
    "with open('data/share_gpt_512_llama.pkl', 'rb') as file: \n",
    "  all_data = pkl.load(file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14258, 512])\n"
     ]
    }
   ],
   "source": [
    "all_attn = torch.zeros((len(all_data), MAX_LEN))\n",
    "all_inputs = list()\n",
    "idx = 0 \n",
    "\n",
    "for i, attn in all_data: \n",
    "\ttry: \n",
    "\t\tall_attn[idx, -len(attn):] = torch.tensor(attn)\n",
    "\t\tall_inputs.append(i.strip())\n",
    "\t\tidx += 1\n",
    "\texcept: \n",
    "\t\tpass \n",
    "\n",
    "all_attn = all_attn[:len(all_inputs)]\n",
    "assert all_attn.size(0) == len(all_inputs)\n",
    "print(all_attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33muuzall\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ee6596dfd94341aad9632392d9f6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.0166682273499949, max=1.0))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/uzal/New Volume/Programming/instructgpt/wandb/run-20230724_115254-sbrhc6w8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uuzall/Instruct%20Training/runs/sbrhc6w8' target=\"_blank\">fearless-monkey-32</a></strong> to <a href='https://wandb.ai/uuzall/Instruct%20Training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uuzall/Instruct%20Training' target=\"_blank\">https://wandb.ai/uuzall/Instruct%20Training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uuzall/Instruct%20Training/runs/sbrhc6w8' target=\"_blank\">https://wandb.ai/uuzall/Instruct%20Training/runs/sbrhc6w8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "project_name = '' # input() \n",
    "\n",
    "wandb.init(\n",
    "    project='Instruct Training', \n",
    "    entity='uuzall', \n",
    "    sync_tensorboard=True, \n",
    "    name=project_name, \n",
    "    monitor_gym=True, \n",
    "    save_code=True,\n",
    ")\n",
    "\n",
    "writer = torch.utils.tensorboard.SummaryWriter(f'runs/{project_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "def calc_loss(input_ids, logits, attn): \n",
    "\tshift_labels = input_ids[..., 1:].contiguous() \n",
    "\tshift_logits = logits[..., :-1, :].contiguous() \n",
    "\tshift_attn = attn[:, -input_ids.size(1):]\n",
    "\tshift_attn = shift_attn[..., :-1].contiguous()\n",
    "\tloss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\tloss *= shift_attn.view(-1)\n",
    "\treturn loss.sum() / (shift_attn.sum()+1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-5\n",
    "bs, scale_bs = 64, 2\n",
    "steps = bs // scale_bs \n",
    "cutoff = int(len(all_inputs)*0.95)\n",
    "test_loss, best_test_loss = 0, 100\n",
    "n_epochs = 2\n",
    "global_step = 0 \n",
    "train_dl = DataLoader(list(zip(all_inputs[:cutoff], all_attn[:cutoff])), batch_size=scale_bs, shuffle=True, pin_memory=True)\n",
    "test_dl = DataLoader(list(zip(all_inputs[cutoff:], all_attn[cutoff:])), batch_size=scale_bs, shuffle=False, pin_memory=True)\n",
    "\n",
    "optimizer = optim.Adafactor(model.parameters(), scale_parameter=False, relative_step=False, lr=lr)\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.03*n_epochs*len(train_dl)//steps), num_training_steps=(n_epochs*len(train_dl))//steps)\n",
    "\n",
    "accelerator = Accelerator(gradient_accumulation_steps=steps, mixed_precision='bf16') \n",
    "model, optimizer, train_dl, test_dl, scheduler = accelerator.prepare(model, optimizer, train_dl, test_dl, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6773 [00:00<?, ?it/s]/media/uzal/New Volume/python_environments/AI3_11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/media/uzal/New Volume/python_environments/AI3_11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "Epochs: 1/2:  96%|█████████▌| 6475/6773 [2:42:11<07:29,  1.51s/it, best_test_loss=0.865, loss=0.855, test_loss=0.865]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 1/2:  96%|█████████▌| 6476/6773 [2:42:13<07:43,  1.56s/it, best_test_loss=0.865, loss=0.729, test_loss=0.865]"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs): \n",
    "\tfor idx, (x, attn) in (loop := tqdm(enumerate(train_dl), total=len(train_dl))): \n",
    "\t\tinputs = tokenizer(x, return_tensors='pt', max_length=MAX_LEN, padding='longest', truncation=True)\n",
    "\t\tout = model(**inputs.to(device))\n",
    "\t\tloss = calc_loss(inputs.input_ids, out.logits, attn) / steps \n",
    "\t\taccelerator.backward(loss)\n",
    "\n",
    "\t\tif idx % steps == 0: \n",
    "\t\t\toptimizer.step() \n",
    "\t\t\tmodel.zero_grad() \n",
    "\t\t\tscheduler.step() \n",
    "\n",
    "\t\tloop.set_description(f'Epochs: {epoch+1}/{n_epochs}')\n",
    "\t\tloop.set_postfix(loss=loss.item()*steps, test_loss=test_loss, best_test_loss=best_test_loss) \n",
    "\n",
    "\t\twriter.add_scalar('charts/learning_rate', optimizer.param_groups[0]['lr'], global_step)\n",
    "\t\twriter.add_scalar('losses/train_loss', loss.item()*steps, global_step)\n",
    "\n",
    "\t\tif idx % (bs * 10) == 0: \n",
    "\t\t\tmodel.eval() \n",
    "\t\t\ttest_loss = 0 \n",
    "\t\t\twith torch.no_grad(): \n",
    "\t\t\t\tfor (x, attn) in test_dl: \n",
    "\t\t\t\t\tinputs = tokenizer(x, return_tensors=\"pt\", max_length=MAX_LEN, padding='longest', truncation=True)\n",
    "\t\t\t\t\tout = model(**inputs.to(device))\n",
    "\t\t\t\t\ttest_loss += calc_loss(inputs.input_ids, out.logits, attn).item() * scale_bs \n",
    "\t\t\t\t\tloop.set_postfix(loss=loss.item()*steps, test_loss=test_loss, best_test_loss=best_test_loss)\n",
    "\n",
    "\t\t\t\ttest_loss /= (len(all_inputs)-cutoff) \n",
    "\t\t\tif test_loss < best_test_loss: \n",
    "\t\t\t\tbest_test_loss = test_loss \n",
    "\t\t\t\taccelerator.wait_for_everyone() \n",
    "\t\t\t\tunwrapped_model = accelerator.unwrap_model(model)\n",
    "\t\t\t\tunwrapped_model.save_pretrained(f'models/llama_2_best', save_function=accelerator.save, state_dict=accelerator.get_state_dict(model))\n",
    "\n",
    "\t\t\tmodel.train() \n",
    "\t\t\twriter.add_scalar('losses/true_test_loss', test_loss, global_step)\n",
    "\t\tglobal_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 713/713 [02:07<00:00,  5.61it/s, best_test_loss=0.902, loss=1.23, test_loss=643] \n"
     ]
    }
   ],
   "source": [
    "model.eval() \n",
    "test_loss = 0 \n",
    "with torch.no_grad(): \n",
    "\tfor (x, attn) in (loop := tqdm(test_dl)): \n",
    "\t\tinputs = tokenizer(x, return_tensors=\"pt\", max_length=MAX_LEN, padding='longest', truncation=True)\n",
    "\t\tout = model(**inputs.to(device))\n",
    "\t\ttest_loss += calc_loss(inputs.input_ids, out.logits, attn).item() * scale_bs \n",
    "\t\tloop.set_postfix(loss=loss.item()*steps, test_loss=test_loss, best_test_loss=best_test_loss)\n",
    "\n",
    "\ttest_loss /= (len(all_data)-cutoff)\n",
    "if test_loss < best_test_loss: \n",
    "\tbest_test_loss = test_loss \n",
    "\taccelerator.wait_for_everyone() \n",
    "\tunwrapped_model = accelerator.unwrap_model(model)\n",
    "\tunwrapped_model.save_pretrained(f'models/llama_2_best', save_function=accelerator.save, state_dict=accelerator.get_state_dict(model))\n",
    "\n",
    "writer.add_scalar('losses/true_test_loss', test_loss, global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Human: Why are anti viral drugs only used for life threatening viruses?\n",
      "Assistant: Anti-viral drugs are not only used for life-threatening viruses. They are also used to treat viral infections that are not life-threatening, such as the common cold. Anti-viral drugs work by interfering with the virus's ability to replicate, which can help to reduce the severity and duration of an infection. However, anti-viral drugs are not effective against all viruses, and they may have side effects or interact with other medications.</s>\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): \n",
    "\tx = '''Human: Why are anti viral drugs only used for life threatening viruses?\n",
    "Assistant:'''\n",
    "# \tx = '''Human: I've made a hub site comprised of various employee moments (from onboarding to offboarding and other life events such as getting married, getting sick, etc. in between). The last employee moment is retirement (which should be separate from offboarding). How do you feel the page should look like?\n",
    "# Assistant:'''\n",
    "# \tx = '''Human: What is potential energy?\n",
    "# Assistant:'''\n",
    "\tinputs = tokenizer(x, return_tensors='pt', max_length=MAX_LEN, padding='longest', truncation=True)\n",
    "\tout = model.generate(**inputs.to(device), max_new_tokens=128, temperature=0.5, do_sample=True)\n",
    "\tprint(tokenizer.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Assistant: The general name of energy which has to do with location relative to something else is called potential energy. In this particular case, of course, we call it gravitational potential energy. If it is a question of electrical forces against which we are working, instead of gravitational forces, if we are “lifting” charges away from other charges with a lot of levers, then the energy content is called electrical potential energy. The general principle is that the change in the energy is the force times the distance that the force is pushed, and that this is a change in energy in general:\n",
      "Fig. 14-3. The potential energy between two atoms as a function of the distance between them.\n",
      "Remember that the potential φ has a physical significance: it is the potential energy which a unit charge would have if brought to the specified point in space from some reference point.\n",
      "etc., which are acting with respect to one another in pairs due to forces all of which are conservative. In these circumstances the kinetic energy in the entire system is simply the sum of the kinetic energies of all of the particular atoms or planets or whatever, and the potential energy of the system is the sum, over the pairs of particles, of the potential energy of mutual interaction of a single pair, as though the others were not there. (This is really not true for molecular forces, and the formula is somewhat more complicated; it certainly is true for Newtonian gravitation, and it is true as an approximation for molecular forces. For molecular forces there is a potential energy, but it is sometimes a more complicated function of the positions of the atoms than simply a sum of terms from pairs.) In the special case of gravity, therefore, the potential energy is the sum, over all the pairs i and j, of Gmimj/rij, as was indicated in Eq. (). Equation () expressed mathematically the following proposition: that the total kinetic energy plus the total potential energy does not change with time. As the various planets wheel about, and turn and twist and so on, if we calculate the total kinetic energy and the total potential energy we find that the total remains constant.</s> \n",
      "\n",
      "Human: According to what you said, what is Potential Energy?\n",
      "Assistant: Potential Energy is the energy that is stored in an object or a system. It is the energy that is stored in an object or a system due to its position, shape, or other factors.\n",
      "\n",
      "Assistant: Potential Energy is the energy that is stored in an object or a system due to its position, shape, or other factors.</s>\n",
      "\n",
      "<s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s> Assistant: be simple. Try to imagine what makes a drag on an airplane flying through the air—the air rushing over the wings, the swirling in the back, the changes going on around the fuselage, and many other complications, and you see that there is not going to be a simple law. On the other hand, it is a remarkable fact that the drag force on an airplane is approximately a constant times the square of the velocity, or F cv2.\n",
      "law that can be used in the design of airplanes, but this law is not in the same class as the basic laws of physics, and further study of it will only make it more and more complicated. A study of how the coefficient c depends on the shape of the front of the airplane is, to put it mildly, frustrating. There just is no simple law for determining the coefficient in terms of the shape of the airplane. In contrast, the law of gravitation is simple, and further study only indicates its greater simplicity.\n",
      "air—they get too heavy to be supported any longer in the updraft. As they come down, they draw a little air with them and start a downdraft. And surprisingly enough, it is easy to see that once the downdraft is started, it will maintain itself. The air now drives itself down!</s> \n",
      "\n",
      "Human: According to what you said, what makes an airplane fly? \n",
      "Assistant: An airplane flies because it is pushed by the air. \n",
      "\n",
      "Assistant: The airplane is pushed by the air, which causes the airplane to accelerate. \n",
      "Assistant: The airplane is pushed by the air, which causes the airplane to accelerate. </s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
      "\n",
      "<s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s> Assistant: Where do the currents come from? One possibility would be from the motion of the electrons in atomic orbits. Actually, that is not the case for iron, although it is for some materials. In addition to moving around in an atom, an electron also spins about on its own axis—something like the spin of the earth—and it is the current from this spin that gives the magnetic field in iron. (We say “something like the spin of the earth” because the question is so deep in quantum mechanics that the classical ideas do not really describe things too well.) In most substances, some electrons spin one way and some spin the other, so the magnetism cancels out, but in iron—for a mysterious reason which we will discuss later—many of the electrons are spinning with their axes lined up, and that is the source of the magnetism.\n",
      "In any case, we have found an induced atomic moment proportional to the magnetic field B and opposing it. This is diamagnetism of matter. It is this magnetic effect that is responsible for the small force on a piece of bismuth in a nonuniform magnetic field. (You could compute the force by working out the energy of the induced moments in the field and seeing how the energy changes as the material is moved into or out of the high-field region.)\n",
      "We find that the induced magnetization—the magnetic moment per unit volume—is proportional to the magnetic field. This is the phenomenon of paramagnetism. You will see that the effect is stronger at lower temperatures and weaker at higher temperatures. When we put a field on a substance, it develops, for small fields, a magnetic moment proportional to the field. The ratio of M to B (for small fields) is called the magnetic susceptibility.</s> \n",
      "\n",
      "Human: According to what you said, what causes magnetism? \n",
      "Assistant: ​\n",
      "Assistant: ​\n",
      "Assistant: ​</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
      "\n",
      "<s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s> Assistant: Hunan First Normal University. Hunan First Normal University, founded in 1903, is a higher education institution located in Yuelu District, Changsha, Hunan Province, China.\n",
      "Profile of Hunan First Normal College. Hunan First Normal College ---the alma mater of Mao Zedong, was founded in. 1903, and can date back to the Nan Song Dynasty when Southern Changsha City. Academy was founded. Now it is a three-year normal college, and enjoying the.\n",
      "Hunan First Normal University covers a total area of 1346 mu, with more than 420,000 square meters of floor space. The university is divided into 10 colleges.\n",
      "Hunan First Normal University. Public University, Changsha City, Hunan province, China. Introduction. Hunan First Normal University - the alma mater of Mao Zedong, was founded in 1903, and can date back to the Nan Song Dynasty when Southern Changsha City Academy was founded.\n",
      "Publish your University Ranking. Established in 1903, Hunan First Normal University is a higher education institution located in the large city of Changsha (population range of 1,000,000-5,000,000 inhabitants), Hunan. Officially accredited/recognized by the Department of Education, Hunan Province, Hunan First Normal University is a coeducational higher education institution.\n",
      "As I couldn't find the Red Hotel in the Changsha hotel section, I thought I'd review it here. It's associated with the University. Just about everything else was poor. The rooms are shabby. They provide minimal toiletries once during a three day stay.</s> \n",
      "\n",
      "Human: According to what you said, How many colleges does hunan first normal university have? \n",
      "Assistant: \n",
      "Assistant: \n",
      "Assistant: \n",
      "Assistant: </s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = [f'''Assistant: The general name of energy which has to do with location relative to something else is called potential energy. In this particular case, of course, we call it gravitational potential energy. If it is a question of electrical forces against which we are working, instead of gravitational forces, if we are “lifting” charges away from other charges with a lot of levers, then the energy content is called electrical potential energy. The general principle is that the change in the energy is the force times the distance that the force is pushed, and that this is a change in energy in general:\n",
    "Fig. 14-3. The potential energy between two atoms as a function of the distance between them.\n",
    "Remember that the potential φ has a physical significance: it is the potential energy which a unit charge would have if brought to the specified point in space from some reference point.\n",
    "etc., which are acting with respect to one another in pairs due to forces all of which are conservative. In these circumstances the kinetic energy in the entire system is simply the sum of the kinetic energies of all of the particular atoms or planets or whatever, and the potential energy of the system is the sum, over the pairs of particles, of the potential energy of mutual interaction of a single pair, as though the others were not there. (This is really not true for molecular forces, and the formula is somewhat more complicated; it certainly is true for Newtonian gravitation, and it is true as an approximation for molecular forces. For molecular forces there is a potential energy, but it is sometimes a more complicated function of the positions of the atoms than simply a sum of terms from pairs.) In the special case of gravity, therefore, the potential energy is the sum, over all the pairs i and j, of Gmimj/rij, as was indicated in Eq. (). Equation () expressed mathematically the following proposition: that the total kinetic energy plus the total potential energy does not change with time. As the various planets wheel about, and turn and twist and so on, if we calculate the total kinetic energy and the total potential energy we find that the total remains constant.{tokenizer.eos_token}\n",
    "\n",
    "Human: According to what you said, what is Potential Energy?\n",
    "Assistant:''',\n",
    "f'''Assistant: be simple. Try to imagine what makes a drag on an airplane flying through the air—the air rushing over the wings, the swirling in the back, the changes going on around the fuselage, and many other complications, and you see that there is not going to be a simple law. On the other hand, it is a remarkable fact that the drag force on an airplane is approximately a constant times the square of the velocity, or F cv2.\n",
    "law that can be used in the design of airplanes, but this law is not in the same class as the basic laws of physics, and further study of it will only make it more and more complicated. A study of how the coefficient c depends on the shape of the front of the airplane is, to put it mildly, frustrating. There just is no simple law for determining the coefficient in terms of the shape of the airplane. In contrast, the law of gravitation is simple, and further study only indicates its greater simplicity.\n",
    "air—they get too heavy to be supported any longer in the updraft. As they come down, they draw a little air with them and start a downdraft. And surprisingly enough, it is easy to see that once the downdraft is started, it will maintain itself. The air now drives itself down!{tokenizer.eos_token}\n",
    "\n",
    "Human: According to what you said, what makes an airplane fly? \n",
    "Assistant:''', \n",
    "f'''Assistant: Where do the currents come from? One possibility would be from the motion of the electrons in atomic orbits. Actually, that is not the case for iron, although it is for some materials. In addition to moving around in an atom, an electron also spins about on its own axis—something like the spin of the earth—and it is the current from this spin that gives the magnetic field in iron. (We say “something like the spin of the earth” because the question is so deep in quantum mechanics that the classical ideas do not really describe things too well.) In most substances, some electrons spin one way and some spin the other, so the magnetism cancels out, but in iron—for a mysterious reason which we will discuss later—many of the electrons are spinning with their axes lined up, and that is the source of the magnetism.\n",
    "In any case, we have found an induced atomic moment proportional to the magnetic field B and opposing it. This is diamagnetism of matter. It is this magnetic effect that is responsible for the small force on a piece of bismuth in a nonuniform magnetic field. (You could compute the force by working out the energy of the induced moments in the field and seeing how the energy changes as the material is moved into or out of the high-field region.)\n",
    "We find that the induced magnetization—the magnetic moment per unit volume—is proportional to the magnetic field. This is the phenomenon of paramagnetism. You will see that the effect is stronger at lower temperatures and weaker at higher temperatures. When we put a field on a substance, it develops, for small fields, a magnetic moment proportional to the field. The ratio of M to B (for small fields) is called the magnetic susceptibility.{tokenizer.eos_token}\n",
    "\n",
    "Human: According to what you said, what causes magnetism? \n",
    "Assistant:''', \n",
    "f'''Assistant: Hunan First Normal University. Hunan First Normal University, founded in 1903, is a higher education institution located in Yuelu District, Changsha, Hunan Province, China.\n",
    "Profile of Hunan First Normal College. Hunan First Normal College ---the alma mater of Mao Zedong, was founded in. 1903, and can date back to the Nan Song Dynasty when Southern Changsha City. Academy was founded. Now it is a three-year normal college, and enjoying the.\n",
    "Hunan First Normal University covers a total area of 1346 mu, with more than 420,000 square meters of floor space. The university is divided into 10 colleges.\n",
    "Hunan First Normal University. Public University, Changsha City, Hunan province, China. Introduction. Hunan First Normal University - the alma mater of Mao Zedong, was founded in 1903, and can date back to the Nan Song Dynasty when Southern Changsha City Academy was founded.\n",
    "Publish your University Ranking. Established in 1903, Hunan First Normal University is a higher education institution located in the large city of Changsha (population range of 1,000,000-5,000,000 inhabitants), Hunan. Officially accredited/recognized by the Department of Education, Hunan Province, Hunan First Normal University is a coeducational higher education institution.\n",
    "As I couldn't find the Red Hotel in the Changsha hotel section, I thought I'd review it here. It's associated with the University. Just about everything else was poor. The rooms are shabby. They provide minimal toiletries once during a three day stay.{tokenizer.eos_token}\n",
    "\n",
    "Human: According to what you said, How many colleges does hunan first normal university have? \n",
    "Assistant:''']\n",
    "\n",
    "with torch.no_grad(): \n",
    "\tinputs = tokenizer(test, return_tensors='pt', max_length=MAX_LEN, padding='longest', truncation=True)\n",
    "\tout = model.generate(**inputs.to(device), max_new_tokens=128, temperature=0.5, do_sample=True)#, length_penalty=-100.0, repetition_penalty=0.01)\n",
    "\td = tokenizer.batch_decode(out)\n",
    "\tfor i in d: \n",
    "\t\tprint(i, end='\\n\\n') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
